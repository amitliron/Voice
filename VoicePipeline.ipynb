{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b0a744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc02b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1f4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os                import walk\n",
    "from pydub             import AudioSegment\n",
    "from pydub.utils       import get_array_type\n",
    "from pydub.utils       import mediainfo\n",
    "from pydub.silence     import split_on_silence\n",
    "from datasets          import load_dataset\n",
    "from torchaudio.utils  import download_asset\n",
    "from scipy             import signal\n",
    "from scipy.io          import wavfile\n",
    "from matplotlib.pyplot import figure\n",
    "from tqdm              import tqdm\n",
    "from os                import listdir\n",
    "from os.path           import isfile, join\n",
    "from datetime          import datetime\n",
    "from pyctcdecode       import build_ctcdecoder\n",
    "from pprint            import pprint\n",
    "from torch.utils.data  import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import soundfile         as sf\n",
    "\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import whisper\n",
    "import git\n",
    "import os\n",
    "import jiwer\n",
    "import IPython\n",
    "import array\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a7a6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23978c2b",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#28AA9C;\"> <center> <a id='start_cell'></a> Table of content: </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e532c89",
   "metadata": {},
   "source": [
    "[Pipeline](#pipeline_cell) </br>\n",
    "[Police Example](#Police_Example) </br>\n",
    "[VAD Threshold paramter example](#Threshold_Example) </br>\n",
    "[Whisper](#Whisper) </br>\n",
    "[Police Whisper](#police_dataset_whisper_cell) </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba6710",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#EBA11F;\"> <center> Hebrew: word reading rate:  </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ef9bb",
   "metadata": {},
   "source": [
    "https://www.globes.co.il/news/article.aspx?did=603261"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4cba4",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\"> 140-150 Words per minute -> seem unreal -> so I took 1.5 words per second </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404ee7d",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> GPU / CPU </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925efa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d408b",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> <a id='pipeline_cell'></a> Pipeline Utils </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaf989",
   "metadata": {},
   "source": [
    "[Table Of Content](#start_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0079ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_speaking_length(speech_timestamps):\n",
    "    total = 0\n",
    "    for val in speech_timestamps:\n",
    "        tmp = val['end'] - val['start']\n",
    "        total = total + tmp\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95bb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1be94b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HebrewNormalizer(hebrew_text):\n",
    "    # --- step 1: remove sign characters\n",
    "    #\n",
    "    # ignore_characters = \",~!@#%^&*()-+/|<>[]*'?.{}\"\n",
    "    # for character in ignore_characters:\n",
    "    #     hebrew_text = hebrew_text.replace(character, '')\n",
    "\n",
    "        # --- step 2: replace signs\n",
    "    hebrew_text = hebrew_text.replace('$', \" דולר\")\n",
    "    hebrew_text = hebrew_text.replace('₪', \" שח\")\n",
    "    hebrew_text = hebrew_text.replace('€', \" יורו\")\n",
    "    # hebrew_text = hebrew_text.replace('.', \" נקודה\")\n",
    "    hebrew_text = hebrew_text.replace('ת\"א', \"תל אביב\")\n",
    "    hebrew_text = hebrew_text.replace('ב\"ש', \"באר שבע\")\n",
    "    hebrew_text = hebrew_text.replace('ע\"י', \"על ידי\")\n",
    "    hebrew_text = hebrew_text.replace('אח\"כ', \"אחר כך\")\n",
    "    hebrew_text = hebrew_text.replace('\\\"', \"\")\n",
    "\n",
    "    # for now we will not handle digits, we will have to handle digits if it costs us in the performance of the model\n",
    "    # TODO: handle dates: 3/7 -> third of july\n",
    "    # # --- step 3: replace numbers to words\n",
    "    # dict_nums = {\n",
    "    #     \"0\": \"אפס\",\n",
    "    #     \"1\": \"אחד\",\n",
    "    #     \"2\": \"שתיים\",\n",
    "    #     \"3\": \"שלוש\",\n",
    "    #     \"4\": \"ארבע\",\n",
    "    #     \"5\": \"חמש\",\n",
    "    #     \"6\": \"שש\",\n",
    "    #     \"7\": \"שבע\",\n",
    "    #     \"8\": \"שמונה\",\n",
    "    #     \"9\": \"תשע\",\n",
    "    #     \"10\": \"עשר\",\n",
    "    # }\n",
    "    # for digit, word in dict_nums.items():\n",
    "    #     hebrew_text = hebrew_text.replace(digit, word)\n",
    "    #\n",
    "    # # --- step 4: replace female numbers to male numbers\n",
    "    # dict_male = {\n",
    "    #     \"אחת\": \"אחד\",\n",
    "    #     \"שתיים\": \"שניים\",\n",
    "    #     \"שלושה\": \"שלוש\",\n",
    "    #     \"ארבעה\": \"ארבע\",\n",
    "    #     \"חמישה\": \"חמש\",\n",
    "    #     \"שישה\": \"שש\",\n",
    "    #     \"שבעה\": \"שבע\",\n",
    "    #     \"תשעה\": \"תשע\",\n",
    "    #\n",
    "    # }\n",
    "    # for female, male in dict_male.items():\n",
    "    #     hebrew_text = hebrew_text.replace(female, male)\n",
    "    # postproccessing, removing special charcteres after handling and translating them\n",
    "    valid_tokens = \"פ ם ן ו ט א ר ק ף ך ל ח י ע כ ג ד ש ץ ת צ מ נ ה ב ס ז 1 2 3 4 5 6 7 8 9 0\"\n",
    "    valid_tokens = set([x.lower() for x in valid_tokens])\n",
    "    # The caret in the character class ([^) means match anything but\n",
    "    invalid_chars_regex = f\"[^\\s{re.escape(''.join(set(valid_tokens)))}]\"\n",
    "\n",
    "\n",
    "    \"\"\" DO ADAPT FOR YOUR USE CASE. this function normalizes the target text. \"\"\"\n",
    "    hebrew_text = re.sub(invalid_chars_regex, \" \", hebrew_text)\n",
    "    hebrew_text = re.sub(\"\\s+\", \" \", hebrew_text).strip()\n",
    "    # --- return result\n",
    "    return hebrew_text    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(vad_model, vad_utils, source_file, dest_file, text, sample_rate, mp3_format=False):\n",
    "    \n",
    "    # 1. get silero util functions\n",
    "    (get_speech_timestamps, save_audio, read_audio,VADIterator, collect_chunks) = vad_utils\n",
    "    \n",
    "    # 2. get file VAD\n",
    "    wav_audio         = read_audio(source_file, sampling_rate=sample_rate)       \n",
    "    full_audio_lenth  = len(wav_audio) / sample_rate\n",
    "    speech_timestamps = get_speech_timestamps(wav_audio, vad_model, sampling_rate=sample_rate, threshold=0.4) # see 'bad example' for threshold\n",
    "    \n",
    "    # 3. calc staicics\n",
    "    new_file_length   = get_total_speaking_length(speech_timestamps) / sample_rate\n",
    "    text_length       = len(text.split())\n",
    "    if text_length < (1.2*new_file_length):\n",
    "        file_name = source_file[source_file.rfind(\"/\")+1:]\n",
    "        print(f\"Outlier: {file_name}, Text length: {text_length}, File Length: {full_audio_lenth} seconds, File Without Silence: {new_file_length}, Required Min Words: {(140/60*new_file_length):.2f}\")\n",
    "        return None, None\n",
    "    \n",
    "    # if we will used mp3\n",
    "    if mp3_format:\n",
    "        # 4. convert to new file (without noise)    \n",
    "        save_audio(\"tmp.wav\",collect_chunks(speech_timestamps, wav_audio), sampling_rate=sample_rate)\n",
    "        \n",
    "        # 5. convert to mp3\n",
    "        AudioSegment.from_wav(\"tmp.wav\").export(dest_file, format=\"mp3\")\n",
    "        os.remove(\"tmp.wav\")\n",
    "    else:\n",
    "        save_audio(dest_file, collect_chunks(speech_timestamps, wav_audio), sampling_rate=sample_rate)\n",
    "\n",
    "    \n",
    "    return text_length, new_file_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed017f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_new_police_data(vad_model, vad_utils, source_folder, dest_folder, sample_rate, gt_excell_path):\n",
    "    \n",
    "    # get poilce GT text\n",
    "    df_gt                  = pd.read_excel(gt_excell_path)\n",
    "    \n",
    "    \n",
    "    # get sub folder list\n",
    "    folder_list   = [f for f in listdir(source_folder) if not isfile(join(source_folder, f))]        \n",
    "    df_statistics = pd.DataFrame()\n",
    "\n",
    "    files_name        = []    \n",
    "    gt_len            = []\n",
    "    file_length       = []      \n",
    "    gt_text           = []\n",
    "    \n",
    "    for folder_name in tqdm(folder_list):\n",
    "        \n",
    "        # --- get wav file\n",
    "        splitted_wav_folder = f\"{source_folder}/{folder_name}\"\n",
    "        splitted_files      = [entry for entry in os.listdir(splitted_wav_folder) if os.path.isfile(os.path.join(splitted_wav_folder, entry))]\n",
    "        \n",
    "        # --- get GT text\n",
    "        gt_list             = df_gt[df_gt['file'] == f\"{folder_name}.mp3\"]['text'].values                \n",
    "                        \n",
    "        for i in range(len(splitted_files)): \n",
    "            part           = \"%04d\" % (i,)\n",
    "            full_src       = f\"{source_folder}/{folder_name}/{folder_name}_{part}.wav\"       \n",
    "            full_dst       = f\"{dest_folder}/{folder_name}_{part}.'wav'\"       \n",
    "            \n",
    "            \n",
    "            text_length, new_file_length = run_pipeline(vad_model, \n",
    "                                                        vad_utils, \n",
    "                                                        full_src, \n",
    "                                                        full_dst, \n",
    "                                                        HebrewNormalizer(gt_list[i]), \n",
    "                                                        sample_rate)\n",
    "            # if outlier\n",
    "            if text_length is None:\n",
    "                continue\n",
    "            \n",
    "            gt_text.append(gt_list[i])\n",
    "            files_name.append(f\"{folder_name}_{part}\")\n",
    "            gt_len.append(text_length)\n",
    "            file_length.append(new_file_length)        \n",
    "                        \n",
    "\n",
    "    df_statistics = pd.DataFrame(data = {\"File\":   files_name,\n",
    "                                  \"Duration\":      file_length,\n",
    "                                  \"Text Length\":   gt_len,\n",
    "                                  \"Text\":          gt_text\n",
    "                                  })                \n",
    "    df_statistics.to_csv(f\"{dest_folder}/statistics.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8863f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f888e6",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> <a id='Police_Example'></a>  Example on new police corpus </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2ef71",
   "metadata": {},
   "source": [
    "[Table Of Content](#start_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8425df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_model, vad_utils = torch.hub.load(repo_or_dir  = 'snakers4/silero-vad',\n",
    "                                      model        = 'silero_vad',\n",
    "                                      force_reload = True,\n",
    "                                      onnx         = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd502f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_excell_path = \"/home/amitli/Datasets/Unit_Tests/new_corpus.xlsx\"\n",
    "source_folder  = \"/home/amitli/Datasets/Unit_Tests/Source\"\n",
    "dest_folder    = \"/home/amitli/Datasets/Unit_Tests/Dest\"\n",
    "sample_rate    = 8000 \n",
    "\n",
    "run_on_new_police_data(vad_model, vad_utils, source_folder, dest_folder, sample_rate, gt_excell_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d157571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/amitli/Datasets/Unit_Tests/Dest/statistics.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfd9b7",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#AA2849;\"> <center> <a id='Threshold_Example'></a> Example why I used threshold=0.4 (instead of 0.5) </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01860204",
   "metadata": {},
   "source": [
    "[Table Of Content](#start_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61911efe",
   "metadata": {},
   "source": [
    "<p style=\"color:green;\">GT Text: \"אולגה ובעלה יהושע אז בעצם מהי\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"/home/amitli/Datasets/Unit_Tests/Source/-37_ZAraHQM/-37_ZAraHQM_0011.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5aa674",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\">Result Text:  missing \"יהושע\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77019777",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_speech_timestamps, save_audio, read_audio,VADIterator, collect_chunks) = vad_utils\n",
    "sample_rate       = 8000\n",
    "wav_audio         = read_audio(\"/home/amitli/Datasets/Unit_Tests/Source/-37_ZAraHQM/-37_ZAraHQM_0011.wav\", sampling_rate=sample_rate)       \n",
    "speech_timestamps = get_speech_timestamps(wav_audio, vad_model, sampling_rate=sample_rate)        \n",
    "save_audio(\"/home/amitli/Datasets/Unit_Tests/tmp.wav\",collect_chunks(speech_timestamps, wav_audio), sampling_rate=sample_rate)\n",
    "IPython.display.Audio(\"/home/amitli/Datasets/Unit_Tests/tmp.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4b582",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">Try to manual fix:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_speech_timestamps, save_audio, read_audio,VADIterator, collect_chunks) = vad_utils\n",
    "sample_rate       = 8000\n",
    "wav_audio         = read_audio(\"/home/amitli/Datasets/Unit_Tests/Source/-37_ZAraHQM/-37_ZAraHQM_0011.wav\", sampling_rate=sample_rate)       \n",
    "speech_timestamps = get_speech_timestamps(wav_audio, vad_model, sampling_rate=sample_rate, threshold=0.4)        \n",
    "save_audio(\"/home/amitli/Datasets/Unit_Tests/tmp.wav\",collect_chunks(speech_timestamps, wav_audio), sampling_rate=sample_rate)\n",
    "IPython.display.Audio(\"/home/amitli/Datasets/Unit_Tests/tmp.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadba37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174e3f7d",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> <a id='Whisper'></a> Whisper </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4001d5",
   "metadata": {},
   "source": [
    "[Table Of Content](#start_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Whisper_Text(whisper_model, file_name, device):\n",
    "    \n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(file_name)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel   = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions(language = 'he', beam_size=8, patience=2, fp16 = False)    #, temperature=0.2\n",
    "    result  = whisper.decode(whisper_model, mel, options)\n",
    "    result  = result.text\n",
    "    result  = HebrewNormalizer(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3491be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"large\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef06f02",
   "metadata": {},
   "source": [
    "<p style=\"color:#28AA9C; font-size:24px;\"> Run on first 2 rows from dataframe and add Whisper results: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df             = pd.read_csv(\"/home/amitli/Datasets/Unit_Tests/Dest/statistics.csv\")\n",
    "whisper_column = [None]*df.shape[0]\n",
    "whisper_wer    = [None]*df.shape[0]\n",
    "\n",
    "for i in range(2):\n",
    "    audio_file        = f\"{dest_folder}/{df['File'].values[i]}.mp3\"\n",
    "    whisper_text      = Get_Whisper_Text(whisper_model, audio_file, \"cpu\")\n",
    "    whisper_column[i] = (whisper_text)\n",
    "    whisper_wer[i]    = jiwer.wer(df[\"Text\"].values[i], whisper_text)\n",
    "    \n",
    "    \n",
    "df[\"Whisper\"] = whisper_column\n",
    "df[\"WER\"]     = whisper_wer\n",
    "\n",
    "df.to_csv(f\"{dest_folder}/whisper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{dest_folder}/whisper.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Whisper_Transcribe(whisper_model, file_name):            \n",
    "    result = whisper_model.transcribe(file_name)\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190e369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee8f436",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#AA2849;\"> <center> <a id='Threshold_Example'></a> How can it be ??? </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "police_source_file = \"/home/amitli/Datasets/ASR_Police/new_corpus/-37_ZAraHQM.mp3\"\n",
    "print(f\"Police File SR: {mediainfo(police_source_file)['sample_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e631b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_file = \"/home/amitli/Datasets/Unit_Tests/Source/-3dd58g7rQQ/-3dd58g7rQQ_0001.wav\"\n",
    "mp3_file       = \"/home/amitli/Datasets/test_mp3.mp3\"\n",
    "wav_file       = \"/home/amitli/Datasets/test_wav.wav\"\n",
    "\n",
    "# 1. get source sample rate:\n",
    "print(f\"Source file SR: {mediainfo(test_full_file)['sample_rate']}\")\n",
    "    \n",
    "\n",
    "# 2. save as mp3\n",
    "run_pipeline(vad_model, vad_utils, test_full_file, mp3_file, \"a b c d e f\", sample_rate=8000, mp3_format=True)\n",
    "print(f\"MP3 file SR: {AudioSegment.from_mp3(mp3_file).frame_rate}\")\n",
    "\n",
    "# 3. save as wav\n",
    "run_pipeline(vad_model, vad_utils, test_full_file, wav_file, \"a b c d e f\", sample_rate=8000, mp3_format=False)\n",
    "print(f\"WAV file SR: {mediainfo(wav_file)['sample_rate']}\")\n",
    "\n",
    "mp3_text = Get_Whisper_Text(whisper_model, mp3_file, \"cpu\")\n",
    "wav_text = Get_Whisper_Text(whisper_model, wav_file, \"cpu\")\n",
    "\n",
    "mp3_wer  = jiwer.wer(df[\"Text\"].values[1], mp3_text)\n",
    "wav_wer  = jiwer.wer(df[\"Text\"].values[1], wav_text)\n",
    "\n",
    "print(f\"MP3 WER: {mp3_wer:.3f}\")\n",
    "print(f\"WAV WER: {wav_wer:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GT : {df['Text'].values[1]}\")\n",
    "print(f\"MP3: {mp3_text}\")\n",
    "print(f\"WAV: {wav_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65537dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_mp3 = HebrewNormalizer(mp3_text)\n",
    "normalize_wav = HebrewNormalizer(wav_text)\n",
    "\n",
    "\n",
    "print(f\"GT : {df['Text'].values[1]}\")\n",
    "print(f\"Normalized MP3: {normalize_mp3}\")\n",
    "print(f\"Normalized WAV: {normalize_wav}\")\n",
    "\n",
    "mp3_wer  = jiwer.wer(df[\"Text\"].values[1], normalize_mp3)\n",
    "wav_wer  = jiwer.wer(df[\"Text\"].values[1], normalize_wav)\n",
    "\n",
    "print(f\"MP3 WER: {mp3_wer:.3f}\")\n",
    "print(f\"WAV WER: {wav_wer:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2af529",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#28AA9C;\"> <center> <a id='police_dataset_whisper_cell'></a> Run Police Whisper </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d265d",
   "metadata": {},
   "source": [
    "[Table Of Content](#start_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9512fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt         = pd.read_excel(\"/home/amitli/Datasets/MyVoiceTests_16SR/new_corpus.xlsx\")\n",
    "source_folder = \"/home/amitli/Datasets/MyVoiceTests_16SR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60598c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliceDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, source_folder, df_gt, device, filter_files=None):        \n",
    "        \n",
    "        self.folder_list   = [f for f in listdir(source_folder) if not isfile(join(source_folder, f))]        \n",
    "        self.l_main_file   = []\n",
    "        self.l_sub_file    = []\n",
    "        self.l_full_path   = []\n",
    "        self.l_gt          = []\n",
    "        self.device        = device\n",
    "\n",
    "        for folder_name in tqdm(self.folder_list):\n",
    "                        \n",
    "            # --- get wav file\n",
    "            splitted_wav_folder = f\"{source_folder}/{folder_name}\"\n",
    "            splitted_files      = [entry for entry in os.listdir(splitted_wav_folder) if os.path.isfile(os.path.join(splitted_wav_folder, entry))]\n",
    "\n",
    "            # --- get GT text\n",
    "            gt_list             = df_gt[df_gt['file'] == f\"{folder_name}.mp3\"]['text'].values                \n",
    "\n",
    "            for i in range(len(splitted_files)): \n",
    "                part           = \"%04d\" % (i,)\n",
    "                full_src       = f\"{source_folder}/{folder_name}/{folder_name}_{part}.wav\"       \n",
    "                \n",
    "                if filter_files is not None:                      \n",
    "                    if full_src not in filter_files:\n",
    "                        continue\n",
    "\n",
    "                self.l_main_file.append(folder_name)\n",
    "                self.l_sub_file.append(f\"{folder_name}_{part}.wav\")\n",
    "                self.l_full_path.append(full_src)                \n",
    "                self.l_gt.append(gt_list[i])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l_gt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        audio_file_path = self.l_full_path[idx]        \n",
    "        audio           = whisper.load_audio(str(audio_file_path))\n",
    "        audio           = whisper.pad_or_trim(audio)\n",
    "        \n",
    "        mel             = whisper.log_mel_spectrogram(audio).to(self.device)        \n",
    "        \n",
    "        sample          = {'mel':      mel, \n",
    "                          'text':      HebrewNormalizer(self.l_gt[idx]), \n",
    "                          'main_file': self.l_main_file[idx],\n",
    "                          'sub_file':  self.l_sub_file[idx],\n",
    "                          'full_path': audio_file_path}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3afd35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 574.29it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = PoliceDataset(source_folder = \"/home/amitli/Datasets/MyVoiceTests_16SR\",\n",
    "                               df_gt  = df_gt,\n",
    "                               device = DEVICE)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "468590b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23596"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0606ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3089b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_whisper_on_rambo(loader, res_file_name, lang):\n",
    "    df = pd.DataFrame()\n",
    "    for batch in tqdm(loader):\n",
    "        \n",
    "        languages        = []\n",
    "        if lang is not None:\n",
    "            languages = [lang]        \n",
    "        mel              = batch['mel']\n",
    "        audio_data       = {'wav': json.dumps(mel.tolist()), 'languages': languages}\n",
    "        gt               = batch['text']\n",
    "\n",
    "        #res              = requests.get('http://10.53.140.33:80/batch_inference/', json=audio_data)\n",
    "        res              = requests.get('http://10.53.140.33:80/batch_inference_beam/', json=audio_data)\n",
    "        res_list         = res.json()[0]\n",
    "\n",
    "        l_wer            = []\n",
    "        l_whisper        = []    \n",
    "        l_res_lang       = []\n",
    "        l_avg_logprob    = []\n",
    "        l_no_speech_prob = []\n",
    "        l_compres_ratio  = []\n",
    "\n",
    "        for i, res in enumerate(res_list):\n",
    "            whisper_text = res['text']\n",
    "            whisper_text = HebrewNormalizer(whisper_text)\n",
    "            l_whisper.append(whisper_text)                \n",
    "            if whisper_text == '':\n",
    "                l_wer.append(1)\n",
    "            else:\n",
    "                l_wer.append(jiwer.wer(whisper_text, gt[i]))\n",
    "            l_res_lang.append(res['language'])\n",
    "            l_avg_logprob.append(res['avg_logprob'])\n",
    "            l_no_speech_prob.append(res['no_speech_prob'])\n",
    "            l_compres_ratio.append(res['compression_ratio'])\n",
    "\n",
    "\n",
    "        df_tmp     = pd.DataFrame({\n",
    "            \"whisper\":           l_whisper,\n",
    "            \"gt\":                gt,\n",
    "            \"wer\":               l_wer,\n",
    "            \"main_file\":         batch['main_file'],\n",
    "            \"sub_file\":          batch['sub_file'],\n",
    "            \"full_path\":         batch['full_path'],\n",
    "            \"detect_lang\":       l_res_lang,\n",
    "            \"avg_logprob\":       l_avg_logprob,\n",
    "            \"no_speech_prob\":    l_no_speech_prob,\n",
    "            \"compression_ratio\": l_compres_ratio,\n",
    "\n",
    "        })\n",
    "        df = pd.concat([df, df_tmp], ignore_index=True)\n",
    "        #df.to_csv(\"/home/amitli/Datasets/whisper_beam_10.csv\")\n",
    "        df.to_csv(res_file_name)\n",
    "        \n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24068f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_naive   = pd.read_csv(\"/home/amitli/Datasets/whisper.csv\")\n",
    "df_beam_5  = pd.read_csv(\"/home/amitli/Datasets/whisper_beam_5.csv\")\n",
    "df_beam_10 = pd.read_csv(\"/home/amitli/Datasets/whisper_beam_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674462dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23596, 23596, 23596)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_naive), len(df_beam_5), len(df_beam_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26041147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.92, 0.92, 0.92)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_hebrew   = round(np.sum(df_naive['detect_lang'].values == 'he')/len(df_naive), 2)\n",
    "beam_5_hebrew  = round(np.sum(df_beam_5['detect_lang'].values == 'he')/len(df_beam_5), 2)\n",
    "beam_10_hebrew = round(np.sum(df_beam_10['detect_lang'].values == 'he')/len(df_beam_10), 2)\n",
    "naive_hebrew, beam_5_hebrew, beam_10_hebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bbf693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.54, 0.47, 0.46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_wer    = round(np.mean(df_naive[(df_naive['detect_lang'].values == 'he')]['wer'].values), 2)\n",
    "beam_5_wer   = round(np.mean(df_beam_5[(df_beam_5['detect_lang'].values == 'he')]['wer'].values), 2)\n",
    "beam_10_wer  = round(np.mean(df_beam_10[(df_beam_10['detect_lang'].values == 'he')]['wer'].values), 2)\n",
    "\n",
    "naive_wer, beam_5_wer, beam_10_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0fc5c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27, 0.31, 0.32, 6461, 7377, 7548)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_under_30    = round(np.sum(df_naive['wer'].values < 0.3)/len(df_naive), 2)\n",
    "beam_5_under_30   = round(np.sum(df_beam_5['wer'].values < 0.3)/len(df_beam_5), 2)\n",
    "beam_10_under_30  = round(np.sum(df_beam_10['wer'].values < 0.3)/len(df_beam_10), 2)\n",
    "\n",
    "c_naive_under_30    = int(np.sum(df_naive['wer'].values < 0.3))\n",
    "c_beam_5_under_30   = int(np.sum(df_beam_5['wer'].values < 0.3))\n",
    "c_beam_10_under_30  = int(np.sum(df_beam_10['wer'].values < 0.3))\n",
    "\n",
    "naive_under_30, beam_5_under_30, beam_10_under_30, c_naive_under_30, c_beam_5_under_30, c_beam_10_under_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ef9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03, 0.04, 0.04)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_under_5    = round(np.sum(df_naive['wer'].values < 0.05)/len(df_naive), 2)\n",
    "beam_5_under_5   = round(np.sum(df_beam_5['wer'].values < 0.05)/len(df_beam_5), 2)\n",
    "beam_10_under_5  = round(np.sum(df_beam_10['wer'].values < 0.05)/len(df_beam_10), 2)\n",
    "\n",
    "naive_under_5, beam_5_under_5, beam_10_under_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131eca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Naive</th>\n",
       "      <th>Beam_5</th>\n",
       "      <th>Beam_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hebrew</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WER</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WER under 30%</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total under 30%</td>\n",
       "      <td>6461.00</td>\n",
       "      <td>7377.00</td>\n",
       "      <td>7548.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WER under 5%</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type    Naive   Beam_5  Beam_10\n",
       "0           Hebrew     0.92     0.92     0.92\n",
       "1              WER     0.54     0.47     0.46\n",
       "2    WER under 30%     0.27     0.31     0.32\n",
       "3  Total under 30%  6461.00  7377.00  7548.00\n",
       "4     WER under 5%     0.03     0.04     0.04"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results          = pd.DataFrame()\n",
    "df_results[\"Type\"]     = [\"Hebrew\", \"WER\", \"WER under 30%\", \"Total under 30%\", \"WER under 5%\"]\n",
    "df_results[\"Naive\"]    = [naive_hebrew, naive_wer, naive_under_30, c_naive_under_30, naive_under_5]\n",
    "df_results[\"Beam_5\"]   = [beam_5_hebrew, beam_5_wer, beam_5_under_30, c_beam_5_under_30, beam_5_under_5]\n",
    "df_results[\"Beam_10\"]  = [beam_10_hebrew, beam_10_wer, beam_10_under_30, c_beam_10_under_30, beam_10_under_5]\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb07e78",
   "metadata": {},
   "source": [
    "<u> <p style=\"color:blue; font-size:24px\"> Hebrew Tests:</p> </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "791d9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_whisper_on_none_heb = False\n",
    "\n",
    "if run_whisper_on_none_heb is True:\n",
    "    test_not_heb   = df_beam_10[df_beam_10['detect_lang'] != 'he']['full_path'].values[:20]\n",
    "    dataset_no_heb = PoliceDataset(source_folder = \"/home/amitli/Datasets/MyVoiceTests_16SR\",\n",
    "                                   df_gt         = df_gt,\n",
    "                                   device        = DEVICE, \n",
    "                                   filter_files  = test_not_heb)\n",
    "    loader_no_heb  = DataLoader(dataset_no_heb, batch_size=20, shuffle=False)\n",
    "\n",
    "    run_whisper_on_rambo(loader_no_heb, \"/home/amitli/Datasets/whisper_beam_10_heb.csv\", \"he\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41d6a336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_heb = pd.read_csv(\"/home/amitli/Datasets/whisper_beam_10_heb.csv\")\n",
    "df_no_heb['whisper'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87ddc9",
   "metadata": {},
   "source": [
    "<u> <p style=\"color:blue; font-size:24px\"> First & Last word </p> </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052d611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38, 0.32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_equal = 0\n",
    "last_equal  = 0\n",
    "total       = 0\n",
    "\n",
    "for i in range(len(df_beam_10)):\n",
    "    \n",
    "    if df_beam_10['detect_lang'].values[i]  != \"he\":\n",
    "        continue\n",
    "        \n",
    "    whisper = str(df_beam_10[\"whisper\"].values[i])\n",
    "    if len(whisper.split()) <= 1:\n",
    "        continue\n",
    "        \n",
    "    total         = total + 1\n",
    "    \n",
    "    first_gt_word = df_beam_10['gt'].values[i].split(\" \")[0]\n",
    "    last_gt_word  = df_beam_10['gt'].values[i].split(\" \")[-1]\n",
    "               \n",
    "    first_whisper_word = df_beam_10['whisper'].values[i].split(\" \")[0]\n",
    "    last_whisper_word  = df_beam_10['whisper'].values[i].split(\" \")[-1]\n",
    "               \n",
    "    if first_gt_word == first_whisper_word:\n",
    "        first_equal = first_equal + 1\n",
    "               \n",
    "    if last_gt_word == last_whisper_word:\n",
    "        last_equal = last_equal + 1\n",
    "\n",
    "first_equal = round(first_equal/total, 2)\n",
    "last_equal  = round(last_equal/total, 2)\n",
    "\n",
    "first_equal, last_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d78aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2041ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
