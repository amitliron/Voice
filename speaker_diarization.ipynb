{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc63ea7a",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Links </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c01232",
   "metadata": {},
   "source": [
    "https://medium.com/saarthi-ai/who-spoke-when-build-your-own-speaker-diarization-module-from-scratch-e7d725ee279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b54c6",
   "metadata": {},
   "source": [
    "https://github.com/PaddlePaddle/PaddleSpeech/issues/1426"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e2fb7",
   "metadata": {},
   "source": [
    "https://notebook.community/pyannote/pyannote-audio/notebooks/introduction_to_pyannote_audio_speaker_diarization_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7960e",
   "metadata": {},
   "source": [
    "https://medium.com/ekohe/understanding-ai-who-said-what-when-ff24bd56ae43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb274c3",
   "metadata": {},
   "source": [
    "librosa==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66309d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b011e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "716ddee9",
   "metadata": {},
   "source": [
    "English:\n",
    "    [0:0:30-0:2:0] - 3 speakers:\n",
    "https://www.youtube.com/watch?v=b2_ZZ2UpSzI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78049ed5",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Utilities </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d8b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   resemblyzer     import preprocess_wav, trim_long_silences, normalize_volume\n",
    "from   resemblyzer     import VoiceEncoder\n",
    "from   pydub           import AudioSegment\n",
    "from   pydub.utils     import mediainfo\n",
    "from   spectralcluster import SpectralClusterer\n",
    "from   umap            import UMAP\n",
    "\n",
    "import plotly.express  as px\n",
    "import soundfile       as sf\n",
    "import numpy           as np\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE           = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "HEB_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1.wav\"\n",
    "\n",
    "HEB_FILE_PART_A  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1a.wav\"\n",
    "HEB_FILE_PART_B  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1b.wav\"\n",
    "\n",
    "ENG_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/3-speakers.wav\"\n",
    "ENG_FILE_SMALL   = \"/home/amitli/Datasets/speaker-diarization/3-speakers-small.wav\"\n",
    "\n",
    "ENG_YB_FULL      = \"/home/amitli/Datasets/speaker-diarization/English/conversation.wav\"\n",
    "ENG_YB_SMALL     = \"/home/amitli/Datasets/speaker-diarization/English/small-conv.wav\"\n",
    "\n",
    "NEWS             = \"/home/amitli/Datasets/speaker-diarization/English-News/News.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_16sr_file(NEWS, NEWS)\n",
    "#get_sample_rate(NEWS)\n",
    "#IPython.display.Audio(HEB_FILE_FULL)\n",
    "#get_part_of_wav(HEB_FILE_FULL, 32, 92, HEB_FILE_PART_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_16sr_file(source_path, dest_path):    \n",
    "    speech, sr = librosa.load(source_path, sr=16000)\n",
    "    sf.write(dest_path, speech, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_wav(file_path, start_time_sec, end_time_sec, new_file_path):    \n",
    "    t1       = start_time_sec * 1000 \n",
    "    t2       = end_time_sec * 1000\n",
    "    newAudio = AudioSegment.from_wav(file_path)    \n",
    "    newAudio = newAudio[t1:t2]        \n",
    "    newAudio.export(new_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_rate(file):\n",
    "    info          = mediainfo(file)\n",
    "    sampling_rate = info['sample_rate']\n",
    "    sampling_rate = int(sampling_rate)\n",
    "    return sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_rate(ENG_FILE_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b2ee7",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Code </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step I - VAD + normalize audio\n",
    "wav     = preprocess_wav(HEB_FILE_FULL)\n",
    "encoder = VoiceEncoder(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step II: segments + MFCC + embedding\n",
    "_, cont_embeds, wav_splits = encoder.embed_utterance(wav, return_partials=True, rate=16)\n",
    "print(cont_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap           = UMAP()\n",
    "umap_embedding = umap.fit_transform(cont_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=umap_embedding[:, 0], y=umap_embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9734f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectralcluster import RefinementOptions\n",
    "from spectralcluster import ThresholdType\n",
    "from spectralcluster import ICASSP2018_REFINEMENT_SEQUENCE\n",
    "\n",
    "\n",
    "refinement_options = RefinementOptions(\n",
    "    gaussian_blur_sigma          = 1,\n",
    "    p_percentile                 = 0.95,\n",
    "    thresholding_soft_multiplier = 0.01,\n",
    "    thresholding_type            = ThresholdType.RowMax,\n",
    "    refinement_sequence          = ICASSP2018_REFINEMENT_SEQUENCE)\n",
    "\n",
    "clusterer = SpectralClusterer(\n",
    "                              min_clusters       = 1,\n",
    "                              max_clusters       = 5,\n",
    "                              refinement_options = refinement_options)\n",
    "\n",
    "labels = clusterer.predict(cont_embeds)\n",
    "print(f\"labels: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labelling(labels,wav_splits, sampling_rate):\n",
    "\n",
    "    times = [((s.start + s.stop) / 2) / sampling_rate for s in wav_splits]\n",
    "    labelling = []\n",
    "    start_time = 0\n",
    "\n",
    "    for i,time in enumerate(times):\n",
    "        if i>0 and labels[i]!=labels[i-1]:\n",
    "            temp = [str(labels[i-1]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "            start_time = time\n",
    "        if i==len(times)-1:\n",
    "            temp = [str(labels[i]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "\n",
    "#     for cluster, start, end in labelling:\n",
    "#         start = start / sampling_rate\n",
    "#         end   = end   / sampling_rate\n",
    "            \n",
    "    return labelling\n",
    "  \n",
    "labelling = create_labelling(labels,wav_splits, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a82e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(TMP_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826db5f",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> pyannote - Token  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython.display.Audio(ENG_YB_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "MY_TOKEN         = \"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\"\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",use_auth_token=MY_TOKEN)\n",
    "diarization = pipeline(ENG_YB_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = [{\"start\":0.5, \"stop\":18.8, \"speaker\":\"SPEAKER_02\"}, \n",
    "#     {\"start\":21.0, \"stop\":44.1, \"speaker\" :\"SPEAKER_03\"},\n",
    "#     {\"start\":43.4, \"stop\":55.2, \"speaker\" : \"SPEAKER_01\"},\n",
    "#     {\"start\":56.9, \"stop\":68.5, \"speaker\" : \"SPEAKER_03\"},\n",
    "#     {\"start\":70.2, \"stop\":83.9, \"speaker\" : \"SPEAKER_03\"},\n",
    "#     {\"start\":85.5, \"stop\":105.0, \"speaker\" : \"SPEAKER_03\"},\n",
    "#     {\"start\":107.5, \"stop\":114.5, \"speaker\" : \"SPEAKER_02\"},\n",
    "#     {\"start\":115.9, \"stop\":254.7, \"speaker\" : \"SPEAKER_04\"},\n",
    "#     {\"start\":255.0, \"stop\":260.9, \"speaker\" : \"SPEAKER_02\"},\n",
    "#     {\"start\":263.6, \"stop\":327.0, \"speaker\" : \"SPEAKER_00\"}]\n",
    "\n",
    "res = [\n",
    "{\"start\":0.5, \"stop\":0.7, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":1.6, \"stop\":3.4, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":4.1, \"stop\":7.5, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":4.4, \"stop\":5.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":6.1, \"stop\":11.4, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":11.4, \"stop\":13.3, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":13.3, \"stop\":18.3, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":17.5, \"stop\":19.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":19.8, \"stop\":31.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":23.0, \"stop\":24.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":31.8, \"stop\":33.2, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":33.2, \"stop\":35.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":35.0, \"stop\":35.7, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":35.7, \"stop\":38.5, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":39.9, \"stop\":42.3, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":42.9, \"stop\":56.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":56.2, \"stop\":61.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":61.4, \"stop\":63.0, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":63.4, \"stop\":64.4, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":64.5, \"stop\":64.8, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":65.3, \"stop\":66.7, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":67.0, \"stop\":76.4, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":76.5, \"stop\":78.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":78.1, \"stop\":83.0, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":82.9, \"stop\":85.8, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":85.8, \"stop\":86.2, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":86.7, \"stop\":89.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":88.9, \"stop\":88.9, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":90.4, \"stop\":91.5, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":92.4, \"stop\":94.8, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":95.5, \"stop\":110.8, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":98.4, \"stop\":98.4, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":99.1, \"stop\":99.2, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":111.7, \"stop\":121.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":121.7, \"stop\":122.4, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":124.0, \"stop\":128.0, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":129.0, \"stop\":130.8, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":131.7, \"stop\":132.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":133.1, \"stop\":133.6, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":134.6, \"stop\":137.4, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":137.4, \"stop\":139.0, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":139.3, \"stop\":145.5, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":145.5, \"stop\":151.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":146.7, \"stop\":148.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":151.0, \"stop\":155.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":153.7, \"stop\":157.4, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":157.3, \"stop\":164.2, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":162.2, \"stop\":167.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":166.8, \"stop\":171.2, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":168.9, \"stop\":172.9, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":173.9, \"stop\":174.5, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":174.7, \"stop\":175.2, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":175.3, \"stop\":179.1, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":178.0, \"stop\":186.9, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":186.9, \"stop\":207.9, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":190.9, \"stop\":192.9, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":194.9, \"stop\":195.1, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":200.5, \"stop\":201.0, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":208.4, \"stop\":209.7, \"speaker\":\"SPEAKER_00\"},\n",
    "{\"start\":209.7, \"stop\":212.0, \"speaker\":\"SPEAKER_01\"},\n",
    "{\"start\":212.8, \"stop\":214.3, \"speaker\":\"SPEAKER_01\"}]\n",
    "\n",
    "\n",
    "\n",
    "for i, val in enumerate(res):\n",
    "    start = val['start']\n",
    "    end = val['stop']\n",
    "    speaker = val['speaker']\n",
    "    full_path = f\"/home/amitli/Datasets/speaker-diarization/Barkony/{speaker}/{i}.wav\"\n",
    "    get_part_of_wav(HEB_FILE_FULL, start, end, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84835d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69bd0159",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> From Medium  </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13fae2",
   "metadata": {},
   "source": [
    "https://medium.com/ekohe/understanding-ai-who-said-what-when-ff24bd56ae43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b05453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from itertools import groupby\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from huggingface_hub import HfApi\n",
    "# from pyannote.audio import Pipeline\n",
    "# from pyannote.core import Annotation, Segment\n",
    "# from pydub         import AudioSegment\n",
    "# from transformers import HubertForCTC, Wav2Vec2Processor\n",
    "\n",
    "# MY_TOKEN       = \"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\"\n",
    "# audio_filepath = HEB_FILE_PART_A\n",
    "# # pipeline       = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=MY_TOKEN)\n",
    "# # print(type(pipeline))\n",
    "# # output         = pipeline(audio_filepath)\n",
    "# # output\n",
    "\n",
    "# pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=MY_TOKEN)\n",
    "# output   = pipeline(HEB_FILE_PART_A)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc813e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(ENG_YB_FULL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
