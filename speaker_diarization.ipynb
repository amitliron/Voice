{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc63ea7a",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Links </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c01232",
   "metadata": {},
   "source": [
    "https://medium.com/saarthi-ai/who-spoke-when-build-your-own-speaker-diarization-module-from-scratch-e7d725ee279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b54c6",
   "metadata": {},
   "source": [
    "https://github.com/PaddlePaddle/PaddleSpeech/issues/1426"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e2fb7",
   "metadata": {},
   "source": [
    "https://notebook.community/pyannote/pyannote-audio/notebooks/introduction_to_pyannote_audio_speaker_diarization_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7960e",
   "metadata": {},
   "source": [
    "https://medium.com/ekohe/understanding-ai-who-said-what-when-ff24bd56ae43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb274c3",
   "metadata": {},
   "source": [
    "librosa==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66309d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b011e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze |grep librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ddee9",
   "metadata": {},
   "source": [
    "English:\n",
    "    [0:0:30-0:2:0] - 3 speakers:\n",
    "https://www.youtube.com/watch?v=b2_ZZ2UpSzI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78049ed5",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Utilities </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d8b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   resemblyzer     import preprocess_wav, trim_long_silences, normalize_volume\n",
    "from   resemblyzer     import VoiceEncoder\n",
    "from   pydub           import AudioSegment\n",
    "from   pydub.utils     import mediainfo\n",
    "from   spectralcluster import SpectralClusterer\n",
    "from   umap            import UMAP\n",
    "from   pathlib         import Path\n",
    "\n",
    "import plotly.express  as px\n",
    "import soundfile       as sf\n",
    "import numpy           as np\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE           = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "HEB_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1.wav\"\n",
    "\n",
    "HEB_FILE_PART_A  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1a.wav\"\n",
    "HEB_FILE_PART_B  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1b.wav\"\n",
    "\n",
    "ENG_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/3-speakers.wav\"\n",
    "ENG_FILE_SMALL   = \"/home/amitli/Datasets/speaker-diarization/3-speakers-small.wav\"\n",
    "\n",
    "ENG_YB_FULL      = \"/home/amitli/Datasets/speaker-diarization/English/conversation.wav\"\n",
    "ENG_YB_SMALL     = \"/home/amitli/Datasets/speaker-diarization/English/small-conv.wav\"\n",
    "\n",
    "NEWS             = \"/home/amitli/Datasets/speaker-diarization/English-News/News.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"asd\\tsadsad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_16sr_file(NEWS, NEWS)\n",
    "#get_sample_rate(NEWS)\n",
    "#IPython.display.Audio(HEB_FILE_FULL)\n",
    "#get_part_of_wav(HEB_FILE_FULL, 32, 92, HEB_FILE_PART_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_16sr_file(source_path, dest_path):    \n",
    "    speech, sr = librosa.load(source_path, sr=16000)\n",
    "    sf.write(dest_path, speech, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_wav(file_path, start_time_sec, end_time_sec, new_file_path):    \n",
    "    t1       = start_time_sec * 1000 \n",
    "    t2       = end_time_sec * 1000\n",
    "    newAudio = AudioSegment.from_wav(file_path)    \n",
    "    newAudio = newAudio[t1:t2]        \n",
    "    newAudio.export(new_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_rate(file):\n",
    "    info          = mediainfo(file)\n",
    "    sampling_rate = info['sample_rate']\n",
    "    sampling_rate = int(sampling_rate)\n",
    "    return sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_rate(ENG_FILE_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b2ee7",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Code </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step I - VAD + normalize audio\n",
    "wav     = preprocess_wav(HEB_FILE_FULL)\n",
    "encoder = VoiceEncoder(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step II: segments + MFCC + embedding\n",
    "_, cont_embeds, wav_splits = encoder.embed_utterance(wav, return_partials=True, rate=16)\n",
    "print(cont_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap           = UMAP()\n",
    "umap_embedding = umap.fit_transform(cont_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=umap_embedding[:, 0], y=umap_embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9734f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectralcluster import RefinementOptions\n",
    "from spectralcluster import ThresholdType\n",
    "from spectralcluster import ICASSP2018_REFINEMENT_SEQUENCE\n",
    "\n",
    "\n",
    "refinement_options = RefinementOptions(\n",
    "    gaussian_blur_sigma          = 1,\n",
    "    p_percentile                 = 0.95,\n",
    "    thresholding_soft_multiplier = 0.01,\n",
    "    thresholding_type            = ThresholdType.RowMax,\n",
    "    refinement_sequence          = ICASSP2018_REFINEMENT_SEQUENCE)\n",
    "\n",
    "clusterer = SpectralClusterer(\n",
    "                              min_clusters       = 1,\n",
    "                              max_clusters       = 5,\n",
    "                              refinement_options = refinement_options)\n",
    "\n",
    "labels = clusterer.predict(cont_embeds)\n",
    "print(f\"labels: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labelling(labels,wav_splits, sampling_rate):\n",
    "\n",
    "    times = [((s.start + s.stop) / 2) / sampling_rate for s in wav_splits]\n",
    "    labelling = []\n",
    "    start_time = 0\n",
    "\n",
    "    for i,time in enumerate(times):\n",
    "        if i>0 and labels[i]!=labels[i-1]:\n",
    "            temp = [str(labels[i-1]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "            start_time = time\n",
    "        if i==len(times)-1:\n",
    "            temp = [str(labels[i]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "\n",
    "#     for cluster, start, end in labelling:\n",
    "#         start = start / sampling_rate\n",
    "#         end   = end   / sampling_rate\n",
    "            \n",
    "    return labelling\n",
    "  \n",
    "labelling = create_labelling(labels,wav_splits, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a82e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(TMP_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826db5f",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> pyannote - Token  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(NEWS)\n",
    "#IPython.display.Audio(HEB_FILE_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_16sr_file(Yom_Kippur, Yom_Kippur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56918065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "MY_TOKEN    = \"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\"\n",
    "pipeline    = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",use_auth_token=MY_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9847216",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yom_Kippur = \"/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Yom_Kippur_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pipeline(Yom_Kippur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868dc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_speakers(pyannote_diarization_res, src_file, dst_folder):\n",
    "    sr = get_sample_rate(src_file)\n",
    "    if sr != 16000:\n",
    "        print(f\"Sample Rate ({sr})!= 16000\")\n",
    "        return\n",
    "        \n",
    "    i = 1\n",
    "    for turn, _, speaker in pyannote_diarization_res.itertracks(yield_label=True):\n",
    "        start     = turn.start\n",
    "        end       = turn.end\n",
    "        speaker   = speaker\n",
    "        full_path = f\"{dst_folder}/{i}_{speaker}.wav\"        \n",
    "        fold_path = f\"{dst_folder}/{speaker}/{i}_{speaker}.wav\"        \n",
    "        i         = i + 1        \n",
    "        get_part_of_wav(src_file, start, end, full_path)\n",
    "        \n",
    "        \n",
    "Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        get_part_of_wav(src_file, start, end, fold_path)\n",
    "        print(fold_path)\n",
    "        \n",
    "    with open(f\"{dst_folder}/audio.rttm\", \"w\") as f:\n",
    "        pyannote_diarization_res.write_rttm(f)    \n",
    "    \n",
    "\n",
    "seperate_speakers(pyannote_diarization_res = diarization, \n",
    "                  src_file                 = Yom_Kippur,\n",
    "                  dst_folder               = \"/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Results\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "#     print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84835d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69bd0159",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> pyannote/segmentation  </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13fae2",
   "metadata": {},
   "source": [
    "https://huggingface.co/pyannote/segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc813e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(ENG_YB_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815a76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b25a365",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> Google - uis-rnn  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621044cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.audio.features import Precomputed\n",
    "# from pyannote.audio.embedding.utils import simple_label\n",
    "# from pyannote.audio.signal import Binarize\n",
    "# from pyannote.audio.segmentation import Segmentation\n",
    "# from pyannote.database import get_protocol\n",
    "# from pyannote.core import Annotation\n",
    "\n",
    "# # Load pre-trained UIS-RNN model\n",
    "# from pyannote.audio.features import Precomputed\n",
    "# from pyannote.audio.embedding import UIS\n",
    "# pretrained_model = UIS(\n",
    "#     epoch=140, step=150000,\n",
    "#     min_duration=0.500, collar=0.250, threshold=0.5,\n",
    "#     device='cpu'\n",
    "# )\n",
    "\n",
    "# # Load input audio file\n",
    "# audio_file = \"/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Yom_Kippur_1.wav\"\n",
    "\n",
    "# # Extract embeddings from input audio file\n",
    "# precomputed = Precomputed('path/to/precomputed/features/')\n",
    "# embeddings = precomputed.crop(\n",
    "#     uri=audio_file, \n",
    "#     segment=Segment(0, None)\n",
    "# )\n",
    "# embeddings = pretrained_model.apply_embeddings(embeddings)\n",
    "\n",
    "# # Apply clustering to the embeddings\n",
    "# labels = pretrained_model.apply_labels(embeddings)\n",
    "# binary = Binarize(offset=0.5, onset=0.5, log_scale=True, min_duration=0.050)\n",
    "# speech = binary.apply(labels, dimension=1)\n",
    "# partition = Segmentation('audio').apply(speech, dimension=1)\n",
    "\n",
    "# # Convert partition to Annotation\n",
    "# annotation = Annotation()\n",
    "# for segment, label in partition.itertracks(yield_label=True):\n",
    "#     annotation[segment] = simple_label(label)\n",
    "\n",
    "# # Print the speaker diarization results\n",
    "# print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5d6d7",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Streamming - use diart </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb0eb3",
   "metadata": {},
   "source": [
    "https://github.com/juanmc2005/StreamingSpeakerDiarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a082e3c",
   "metadata": {},
   "source": [
    "https://wq2012.github.io/awesome-diarization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7321a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e8a66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyannote.audio==2.1.1\n",
      "pyannote.core==4.5\n",
      "pyannote.database==4.1.3\n",
      "pyannote.metrics==3.2.1\n",
      "pyannote.pipeline==2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze |grep pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ba115a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OnlineSpeakerDiarization' object has no attribute 'from_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m duration, step, latency, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m16000\u001b[39m\n\u001b[1;32m      8\u001b[0m mic \u001b[38;5;241m=\u001b[39m MicrophoneAudioSource(sample_rate)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mOnlineSpeakerDiarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_source\u001b[49m(mic)\u001b[38;5;241m.\u001b[39mpipe(buffer_ouput(duration, step, latency, sample_rate))\u001b[38;5;241m.\u001b[39msubscribe(RealTimePlot(duration, latency))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# mic.read() \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OnlineSpeakerDiarization' object has no attribute 'from_source'"
     ]
    }
   ],
   "source": [
    "from diart.sources   import MicrophoneAudioSource\n",
    "from diart.sinks     import RealTimePlot\n",
    "from diart.operators import buffer_output\n",
    "\n",
    "#from diart.pipelines import OnlineSpeakerDiarization\n",
    "\n",
    "duration, step, latency, sample_rate = 5, 0.5, 0.5, 16000\n",
    "mic = MicrophoneAudioSource(sample_rate)\n",
    "OnlineSpeakerDiarization().from_source(mic).pipe(buffer_ouput(duration, step, latency, sample_rate)).subscribe(RealTimePlot(duration, latency))\n",
    "\n",
    "# mic.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlineSpeakerDiarization()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a86b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diart           import OnlineSpeakerDiarization\n",
    "from diart.sources   import MicrophoneAudioSource\n",
    "from diart.inference import RealTimeInference\n",
    "from diart.sinks     import RTTMWriter\n",
    "\n",
    "#mic       = MicrophoneAudioSource(pipeline.config.sample_rate)\n",
    "#inference = RealTimeInference(pipeline, mic, do_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.attach_observers(RTTMWriter(mic.uri, \"/home/amitli/Downloads/file.rttm\"))\n",
    "prediction = inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992d596",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> VAD vs Speaker Segmentation </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fde569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
