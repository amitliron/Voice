{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc63ea7a",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Links </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c01232",
   "metadata": {},
   "source": [
    "https://medium.com/saarthi-ai/who-spoke-when-build-your-own-speaker-diarization-module-from-scratch-e7d725ee279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b54c6",
   "metadata": {},
   "source": [
    "https://github.com/PaddlePaddle/PaddleSpeech/issues/1426"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e2fb7",
   "metadata": {},
   "source": [
    "https://notebook.community/pyannote/pyannote-audio/notebooks/introduction_to_pyannote_audio_speaker_diarization_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7960e",
   "metadata": {},
   "source": [
    "https://medium.com/ekohe/understanding-ai-who-said-what-when-ff24bd56ae43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb274c3",
   "metadata": {},
   "source": [
    "librosa==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66309d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b011e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze |grep librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ddee9",
   "metadata": {},
   "source": [
    "English:\n",
    "    [0:0:30-0:2:0] - 3 speakers:\n",
    "https://www.youtube.com/watch?v=b2_ZZ2UpSzI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78049ed5",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Utilities </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fdb98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d29a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d8b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be9cbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   resemblyzer     import preprocess_wav, trim_long_silences, normalize_volume\n",
    "from   resemblyzer     import VoiceEncoder\n",
    "from   pydub           import AudioSegment\n",
    "from   pydub.utils     import mediainfo\n",
    "from   spectralcluster import SpectralClusterer\n",
    "from   umap            import UMAP\n",
    "from   pathlib         import Path\n",
    "\n",
    "import plotly.express  as px\n",
    "import soundfile       as sf\n",
    "import numpy           as np\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE           = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "HEB_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1.wav\"\n",
    "\n",
    "HEB_FILE_PART_A  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1a.wav\"\n",
    "HEB_FILE_PART_B  = \"/home/amitli/Datasets/speaker-diarization/Barkony/barkony-1b.wav\"\n",
    "\n",
    "ENG_FILE_FULL    = \"/home/amitli/Datasets/speaker-diarization/3-speakers.wav\"\n",
    "ENG_FILE_SMALL   = \"/home/amitli/Datasets/speaker-diarization/3-speakers-small.wav\"\n",
    "\n",
    "ENG_YB_FULL      = \"/home/amitli/Datasets/speaker-diarization/English/conversation.wav\"\n",
    "ENG_YB_SMALL     = \"/home/amitli/Datasets/speaker-diarization/English/small-conv.wav\"\n",
    "\n",
    "NEWS             = \"/home/amitli/Datasets/speaker-diarization/English-News/News.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_16sr_file(NEWS, NEWS)\n",
    "#get_sample_rate(NEWS)\n",
    "#IPython.display.Audio(HEB_FILE_FULL)\n",
    "#get_part_of_wav(HEB_FILE_FULL, 32, 92, HEB_FILE_PART_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_16sr_file(source_path, dest_path):    \n",
    "    speech, sr = librosa.load(source_path, sr=16000)\n",
    "    sf.write(dest_path, speech, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_wav(file_path, start_time_sec, end_time_sec, new_file_path):    \n",
    "    t1       = start_time_sec * 1000 \n",
    "    t2       = end_time_sec * 1000\n",
    "    newAudio = AudioSegment.from_wav(file_path)    \n",
    "    newAudio = newAudio[t1:t2]        \n",
    "    newAudio.export(new_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f323b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_rate(file):\n",
    "    info          = mediainfo(file)\n",
    "    sampling_rate = info['sample_rate']\n",
    "    sampling_rate = int(sampling_rate)\n",
    "    return sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_rate(ENG_FILE_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b2ee7",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:LightGreen;\"> <center> Code </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step I - VAD + normalize audio\n",
    "wav     = preprocess_wav(HEB_FILE_FULL)\n",
    "encoder = VoiceEncoder(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step II: segments + MFCC + embedding\n",
    "_, cont_embeds, wav_splits = encoder.embed_utterance(wav, return_partials=True, rate=16)\n",
    "print(cont_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap           = UMAP()\n",
    "umap_embedding = umap.fit_transform(cont_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=umap_embedding[:, 0], y=umap_embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9734f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectralcluster import RefinementOptions\n",
    "from spectralcluster import ThresholdType\n",
    "from spectralcluster import ICASSP2018_REFINEMENT_SEQUENCE\n",
    "\n",
    "\n",
    "refinement_options = RefinementOptions(\n",
    "    gaussian_blur_sigma          = 1,\n",
    "    p_percentile                 = 0.95,\n",
    "    thresholding_soft_multiplier = 0.01,\n",
    "    thresholding_type            = ThresholdType.RowMax,\n",
    "    refinement_sequence          = ICASSP2018_REFINEMENT_SEQUENCE)\n",
    "\n",
    "clusterer = SpectralClusterer(\n",
    "                              min_clusters       = 1,\n",
    "                              max_clusters       = 5,\n",
    "                              refinement_options = refinement_options)\n",
    "\n",
    "labels = clusterer.predict(cont_embeds)\n",
    "print(f\"labels: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labelling(labels,wav_splits, sampling_rate):\n",
    "\n",
    "    times = [((s.start + s.stop) / 2) / sampling_rate for s in wav_splits]\n",
    "    labelling = []\n",
    "    start_time = 0\n",
    "\n",
    "    for i,time in enumerate(times):\n",
    "        if i>0 and labels[i]!=labels[i-1]:\n",
    "            temp = [str(labels[i-1]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "            start_time = time\n",
    "        if i==len(times)-1:\n",
    "            temp = [str(labels[i]),start_time,time]\n",
    "            labelling.append(tuple(temp))\n",
    "\n",
    "#     for cluster, start, end in labelling:\n",
    "#         start = start / sampling_rate\n",
    "#         end   = end   / sampling_rate\n",
    "            \n",
    "    return labelling\n",
    "  \n",
    "labelling = create_labelling(labels,wav_splits, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a82e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(TMP_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826db5f",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> pyannote - Token  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(NEWS)\n",
    "#IPython.display.Audio(HEB_FILE_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_16sr_file(Yom_Kippur, Yom_Kippur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56918065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "MY_TOKEN    = \"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\"\n",
    "pipeline    = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",use_auth_token=MY_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9847216",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yom_Kippur = \"/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Yom_Kippur_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "704612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pipeline(Yom_Kippur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868dc107",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_part_of_wav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/audio.rttm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m         pyannote_diarization_res\u001b[38;5;241m.\u001b[39mwrite_rttm(f)    \n\u001b[0;32m---> 23\u001b[0m \u001b[43mseperate_speakers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyannote_diarization_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiarization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msrc_file\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mYom_Kippur\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdst_folder\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mseperate_speakers\u001b[0;34m(pyannote_diarization_res, src_file, dst_folder)\u001b[0m\n\u001b[1;32m     13\u001b[0m fold_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m        \n\u001b[1;32m     14\u001b[0m i         \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m        \n\u001b[0;32m---> 15\u001b[0m \u001b[43mget_part_of_wav\u001b[49m(src_file, start, end, full_path)\n\u001b[1;32m     16\u001b[0m get_part_of_wav(src_file, start, end, fold_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(fold_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_part_of_wav' is not defined"
     ]
    }
   ],
   "source": [
    "def seperate_speakers(pyannote_diarization_res, src_file, dst_folder):\n",
    "    sr = get_sample_rate(src_file)\n",
    "    if sr != 16000:\n",
    "        print(f\"Sample Rate ({sr})!= 16000\")\n",
    "        return\n",
    "        \n",
    "    i = 1\n",
    "    for turn, _, speaker in pyannote_diarization_res.itertracks(yield_label=True):\n",
    "        start     = turn.start\n",
    "        end       = turn.end\n",
    "        speaker   = speaker\n",
    "        full_path = f\"{dst_folder}/{i}_{speaker}.wav\"        \n",
    "        fold_path = f\"{dst_folder}/{speaker}/{i}_{speaker}.wav\"        \n",
    "        i         = i + 1        \n",
    "        get_part_of_wav(src_file, start, end, full_path)\n",
    "        \n",
    "        \n",
    "Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        get_part_of_wav(src_file, start, end, fold_path)\n",
    "        print(fold_path)\n",
    "        \n",
    "    with open(f\"{dst_folder}/audio.rttm\", \"w\") as f:\n",
    "        pyannote_diarization_res.write_rttm(f)    \n",
    "    \n",
    "\n",
    "seperate_speakers(pyannote_diarization_res = diarization, \n",
    "                  src_file                 = Yom_Kippur,\n",
    "                  dst_folder               = \"/home/amitli/Datasets/speaker-diarization/Youtube/Yom_Kippur/Results\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "#     print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84835d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69bd0159",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#F43B76;\"> <center> pyannote/segmentation  </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13fae2",
   "metadata": {},
   "source": [
    "https://huggingface.co/pyannote/segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc813e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython.display.Audio(ENG_YB_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815a76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
