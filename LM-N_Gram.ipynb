{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69100122",
   "metadata": {},
   "source": [
    "https://medium.com/pytorch/working-on-natural-language-processing-nlp-with-pytorch-8090c879aadc\n",
    "\n",
    "https://github.com/L1aoXingyu/pytorch-beginner/blob/master/06-Natural%20Language%20Process/N-Gram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cb6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f5c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765e1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = [((test_sentence[i], test_sentence[i + 1]), test_sentence[i + 2])\n",
    "           for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "vocb = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f26c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e607e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size * n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1)\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_prob = F.log_softmax(out)\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca9950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.346749\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.289707\n",
      "epoch: 3\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_430086/1287663233.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_prob = F.log_softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.233315\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.177454\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.121993\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 5.066857\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 5.011998\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.957275\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.902688\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.848220\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.793885\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.739589\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.685074\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.630446\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 4.575422\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 4.520222\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 4.464723\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 4.408773\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 4.352212\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 4.295164\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 4.237778\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 4.179761\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 4.121441\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 4.062483\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 4.002917\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 3.942937\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 3.882184\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 3.820716\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 3.758732\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 3.696268\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 3.633066\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 3.569583\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 3.505346\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 3.441062\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 3.375884\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 3.310509\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 3.244795\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 3.178486\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 3.111937\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 3.045264\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 2.977965\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 2.910766\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 2.843056\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 2.775212\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 2.707446\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 2.639302\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 2.571321\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 2.503278\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 2.435262\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 2.367592\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 2.300069\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 2.232931\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 2.166173\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 2.099938\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 2.034413\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 1.969541\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 1.905539\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 1.842352\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 1.780162\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 1.719013\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 1.658942\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 1.600031\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 1.542370\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 1.485908\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 1.430792\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 1.377234\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 1.324984\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 1.274266\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 1.225054\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 1.177429\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 1.131364\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 1.086884\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 1.043947\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 1.002663\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 0.962921\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 0.924721\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 0.888084\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 0.852928\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 0.819303\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 0.787136\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 0.756373\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 0.726968\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 0.698984\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 0.672244\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 0.646766\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 0.622538\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 0.599438\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.577501\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.556585\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.536729\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.517821\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.499887\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.482819\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.466627\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.451187\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.436565\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.422663\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.409441\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.396851\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.384897\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('*' * 10)\n",
    "    running_loss = 0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        loss = criterion(out, label)        \n",
    "        running_loss += loss.data.numpy()\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "090ac992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_430086/1287663233.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_prob = F.log_softmax(out)\n"
     ]
    }
   ],
   "source": [
    "word, label = trigram[5]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngrammodel(word)\n",
    "_, predict_label = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806d3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "affd026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real word is And, predict word is And\n"
     ]
    }
   ],
   "source": [
    "predict_word = idx_to_word[predict_label.numpy()[0]]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
